{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import seaborn as sns\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preparedata():\n",
    "    #Load the data\n",
    "    def jsontocsv():\n",
    "        dataset_path = r'C:\\Deepankar\\response_50.json'\n",
    "        with open(dataset_path, encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print('{} docs found in the JSON and type is {}'.format(len(data[\"response\"][\"docs\"]), type(data[\"response\"][\"docs\"])))\n",
    "      \n",
    "        #now will format the data into dataframe - id, pagenum, tag, sentences\n",
    "        try:\n",
    "            sentences = []\n",
    "            for doc in data[\"response\"][\"docs\"]:\n",
    "                id_ = doc[\"id\"]\n",
    "                pagenum = doc[\"pagenumber\"][0]\n",
    "                tag = doc[\"tags\"][0]\n",
    "                \n",
    "                try:\n",
    "                    if doc[\"metainfo\"] is not None:\n",
    "                        for sen in doc[\"metainfo\"]:\n",
    "                            sen = re.sub(r'\\d+', '', sen)\n",
    "                            sen = sen.strip()\n",
    "                            sen = sen.encode('ascii', 'ignore')\n",
    "                            sentences.append([id_, pagenum, tag, sen])\n",
    "                except:\n",
    "                    #print('error in line num {}'.format(id_))\n",
    "                    pass\n",
    "\n",
    "            df = pd.DataFrame(sentences, columns=['id', 'pagenum', 'tag', 'sentence'])\n",
    "            df.to_csv(r'C:\\Deepankar\\NLP\\model\\datafiles\\50.csv')\n",
    "            return df\n",
    "        except:\n",
    "            print('error in line num {}'.format(id_))\n",
    "\n",
    "class getsenembeddings():\n",
    "    \"\"\"\n",
    "    Obtains sentence embeddings for each sentence in the page\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.embed = hub.Module(r\"C:\\Deepankar\\NLP\\model\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\",trainable=True)\n",
    "        self.similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "        self.similarity_message_encodings = self.embed(self.similarity_input_placeholder)\n",
    "        print('About to create tf session..')\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.session.run(tf.tables_initializer())\n",
    "        print('Model Loaded..')\n",
    "        \n",
    "    \n",
    "    def groupsentences(self, df):\n",
    "        data_frame = df\n",
    "        embed = []\n",
    "        for key in df.groupby(['tag','pagenum']).groups.keys():            \n",
    "            print('num of sentences in page {} of book {} is {}'.format(key[1],\n",
    "                                                                        key[0],\n",
    "                                                                        len(df.groupby(['tag','pagenum']).groups[key]))\n",
    "                                                                        )\n",
    "            sen_nums = df.groupby(['tag','pagenum']).groups[key] #This will give index of each sentences         \n",
    "            if len(sen_nums) >= 6:\n",
    "                page, embed = self.pagesenembeddings(sen_nums, df)\n",
    "                embed = embed.reshape(embed.shape[0], -1)                \n",
    "                summary = self.summarize(page, embed)\n",
    "                print('Summary of page {} of book {} is {}'.format(key[1], key[0], summary))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    \n",
    "    def pagesenembeddings(self, sen_nums, df):\n",
    "        print('.....................Embedding of page started.....................')\n",
    "        page_embed = []\n",
    "        page = []\n",
    "        for i in sen_nums:\n",
    "            #print('sentence is {}'.format(str(df.iloc[i]['sentence'])))\n",
    "            page.append(str(df.iloc[i]['sentence']))\n",
    "            page_embed.append(self.session.run(self.similarity_message_encodings, feed_dict =\n",
    "                                             {self.similarity_input_placeholder: [str(df.iloc[i]['sentence'])]}))\n",
    "        page_embed = np.array(page_embed)\n",
    "        \n",
    "        print('.....................next page.....................')\n",
    "        return page, page_embed\n",
    "    \n",
    "    def summarize(self, page, embed):\n",
    "        print('xxxxxxxxxxxxxxxxxxxx Clustering Started xxxxxxxxxxxxxxxxxxxx')\n",
    "        n_clusters = int(np.ceil(len(embed)**0.5))\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "        kmeans = kmeans.fit(embed)\n",
    "        avg = []\n",
    "        closest = []\n",
    "        for j in range(n_clusters):\n",
    "            idx = np.where(kmeans.labels_ == j)[0]\n",
    "            avg.append(np.mean(idx))\n",
    "            \n",
    "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,embed)\n",
    "\n",
    "        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "        summary = ' '.join([page[closest[idx]] for idx in ordering])\n",
    "        \n",
    "        print('xxxxxxxxxxxxxxxxxxxx Clustering Finished xxxxxxxxxxxxxxxxxxxx')\n",
    "        return summary\n",
    "    \n",
    "    def start(self, df):        \n",
    "        self.groupsentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 docs found in the JSON and type is <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pagenum</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b1761496-a33a-40fe-9288-a02685e486f0</td>\n",
       "      <td>7</td>\n",
       "      <td>5950-skip-thought-vectors.pdf</td>\n",
       "      <td>b'and vice-versa for sentences.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b1761496-a33a-40fe-9288-a02685e486f0</td>\n",
       "      <td>7</td>\n",
       "      <td>5950-skip-thought-vectors.pdf</td>\n",
       "      <td>b'We also report the median rank of the closes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b1761496-a33a-40fe-9288-a02685e486f0</td>\n",
       "      <td>7</td>\n",
       "      <td>5950-skip-thought-vectors.pdf</td>\n",
       "      <td>b'Recently,  showed that by using Fishervector...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b1761496-a33a-40fe-9288-a02685e486f0</td>\n",
       "      <td>7</td>\n",
       "      <td>5950-skip-thought-vectors.pdf</td>\n",
       "      <td>b'Thus the method of  is a strong baseline to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1761496-a33a-40fe-9288-a02685e486f0</td>\n",
       "      <td>7</td>\n",
       "      <td>5950-skip-thought-vectors.pdf</td>\n",
       "      <td>b'For our experiments, we represent images usi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  pagenum  \\\n",
       "0  b1761496-a33a-40fe-9288-a02685e486f0        7   \n",
       "1  b1761496-a33a-40fe-9288-a02685e486f0        7   \n",
       "2  b1761496-a33a-40fe-9288-a02685e486f0        7   \n",
       "3  b1761496-a33a-40fe-9288-a02685e486f0        7   \n",
       "4  b1761496-a33a-40fe-9288-a02685e486f0        7   \n",
       "\n",
       "                             tag  \\\n",
       "0  5950-skip-thought-vectors.pdf   \n",
       "1  5950-skip-thought-vectors.pdf   \n",
       "2  5950-skip-thought-vectors.pdf   \n",
       "3  5950-skip-thought-vectors.pdf   \n",
       "4  5950-skip-thought-vectors.pdf   \n",
       "\n",
       "                                            sentence  \n",
       "0                   b'and vice-versa for sentences.'  \n",
       "1  b'We also report the median rank of the closes...  \n",
       "2  b'Recently,  showed that by using Fishervector...  \n",
       "3  b'Thus the method of  is a strong baseline to ...  \n",
       "4  b'For our experiments, we represent images usi...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preparedata.jsontocsv()\n",
    "df.head(5)\n",
    "#df.to_csv(r'C:\\Deepankar\\nlp\\response_1000.csv')\n",
    "obj = getsenembeddings()\n",
    "obj.start(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
